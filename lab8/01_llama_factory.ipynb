{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 Supervised Fine Tuning\n",
    "\n",
    "In this lab, we will perform parameter efficient finetuning (PEFT) to finetune a llama-2 model, using the HuggingFace SFTTrainer tool from its trl library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add proxy to access openai ...\n",
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting trl (from -r requirements.txt (line 1))\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/da/f2/6f47dd96314a281b45695da75e28ece3a9b55931f965587767fc374492a1/trl-0.17.0-py3-none-any.whl (348 kB)\n",
      "Collecting fire (from -r requirements.txt (line 2))\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/6b/b6/82c7e601d6d3c3278c40b7bd35e17e82aa227f050aa9f66cb7b7fce29471/fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.45.5)\n",
      "Collecting deepspeed (from -r requirements.txt (line 4))\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/06/b3/a3903de5c5b707170c5c27e1a40f4ef613f14d241bd84d8b151a2a8786f6/deepspeed-0.16.7.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (13.7.1)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (4.51.0)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 2))\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/4f/bd/de8d508070629b6d84a30d01d57e4a65c69aa7f5abe7560b8fad3b50ea59/termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->-r requirements.txt (line 3)) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (0.8.0)\n",
      "Collecting hjson (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/1f/7f/13cd798d180af4bf4c0ceddeefba2b864a63c71645abc0308b768d67bb81/hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (1.11.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (2.9.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (4.66.5)\n",
      "Collecting nvidia-ml-py (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/db/24/552ebea28f0570b9e65e62b50287a273804c9f997cc1c2dcd4e2d64b9e7d/nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl->-r requirements.txt (line 1)) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl->-r requirements.txt (line 1)) (0.21.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl->-r requirements.txt (line 1)) (2.18.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl->-r requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.16.0)\n",
      "Building wheels for collected packages: fire, deepspeed\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=8d4bb3a2a9560e88a4b1b862743f42e5835593befced351605c3eeab9c91f039\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pmjxnxem/wheels/f7/db/aa/ff6af67e211e1eca763104bbcba0e0feed3830965f1b7f6776\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.16.7-py3-none-any.whl size=1642803 sha256=bc52903adaa761c92574019de7a66b667b99cbb6637de1b780340c55b75e3237\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pmjxnxem/wheels/21/58/7e/1fc0a4ddd1bdc96f8a76751e2a8c8e867ba24551284ef79b07\n",
      "Successfully built fire deepspeed\n",
      "Installing collected packages: nvidia-ml-py, hjson, termcolor, fire, deepspeed, trl\n",
      "Successfully installed deepspeed-0.16.7 fire-0.7.0 hjson-3.1.0 nvidia-ml-py-12.575.51 termcolor-3.1.0 trl-0.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "#!mkdir -p /root/LLM-applications-course/lab8/LLaMA-Factory\n",
    "#!cd /root/LLM-applications-course/lab8/LLaMA-Factory/ && pip install -r /root/LLM-applications-course/lab8/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first change the working directory to /gfshome, to avoid writing too much data to the home directory. (Ignore the warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# copy the config files to /gfshome, the working directory (will need later)\n",
    "!ln -s *.yaml /gfshome/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gfshome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /gfshome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download llama factory\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///gfshome/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (4.51.0)\n",
      "Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.6.0)\n",
      "Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.15.1)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
      "Requirement already satisfied: gradio<=5.25.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (5.23.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.14.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.34.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.115.9)\n",
      "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/c8/48/3e49cf0f64961656402c0023edbc51844fe17afe53ab50e958a6dbbbd499/sse_starlette-2.3.5-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.9.2)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Collecting omegaconf (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
      "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.9.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
      "Collecting av (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/23/42/0eafe0de75de6a0db71add8e4ea51ebf090482bad3068f4a874c90fbd110/av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.8/34.8 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.10.1)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/60/ec/e34d546cfd9c5b906d1d534bb75557be9f2b179609d60bb9e97ec07e8ead/tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
      "Collecting jieba (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-chinese (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/03/0f/394cf877be7b903881020ef7217f7dc644dad158d52a9353fcab22e3464d/rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.10.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.6.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (10.4.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.12.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (2.23.4)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.9.11)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/74/03/3271b7bb470fbab4adf5bd30b0d32143909d96f3608d815b447357f47f2b/shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.0.8)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/3e/38/7859ff46355f76f8d19459005ca000b6e7012f2f1ca597746cbcd1fbfe5e/antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.16.0)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors4.bfsu.edu.cn/pypi/web/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.6)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->llamafactory==0.9.3.dev0) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.0.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Building wheels for collected packages: llamafactory, jieba, antlr4-python3-runtime\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=27215 sha256=7b0f015ecf549350ddd0ee59c03b005b7b80e4a69c6a66da00aff3a6e3119784\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rv0qn5dk/wheels/87/26/82/8f4922c9e797dfc3e05b24c481d0e498ffae7c1e700eb2c667\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=ef91c37ace708843d1660e3543d1e3fe615602efcb938abb708f331fda617134\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rv0qn5dk/wheels/3b/77/58/a8cb380c06e82bb53f260995add8054c0eea214e49282dc7ad\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144551 sha256=9b70b49ea0e1eab5ac43791b9576057f5e120ee94fcb7af58eac3d17c8e41235\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rv0qn5dk/wheels/8d/e7/c3/46be5fce5029db878ef39c1827161c3eb5f5bb90b0efb1abec\n",
      "Successfully built llamafactory jieba antlr4-python3-runtime\n",
      "Installing collected packages: jieba, antlr4-python3-runtime, shtab, rouge-chinese, omegaconf, docstring-parser, av, anyio, tyro, sse-starlette, trl, llamafactory\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.6.0\n",
      "    Uninstalling anyio-4.6.0:\n",
      "      Successfully uninstalled anyio-4.6.0\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.17.0\n",
      "    Uninstalling trl-0.17.0:\n",
      "      Successfully uninstalled trl-0.17.0\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 anyio-4.9.0 av-14.4.0 docstring-parser-0.16 jieba-0.42.1 llamafactory-0.9.3.dev0 omegaconf-2.3.0 rouge-chinese-1.0.3 shtab-1.7.2 sse-starlette-2.3.5 trl-0.9.6 tyro-0.8.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cd LLaMA-Factory && pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Supervised Fine Tuning Example\n",
    "### 2.1 Motivation\n",
    "Llama3 is a versatile large language model available in various parameter sizes. Given its significant improvements in text generation tasks compared to its predecessor, Llama2, we aim to use Llama3-8B-Instruct to generate Chinese poetry based on specific themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Shared parameters between inference and SFT training\n",
    "################################################################################\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "# The base model\n",
    "model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n",
    "# Use a single GPU\n",
    "# device_map = {'':0}\n",
    "# Use all GPUs\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-18 10:12:12,340] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143d271badad403cb90c0511ba12b2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import os\n",
    "os.environ[\"BNB_CUDA_VERSION\"]=\"125\"\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人? [/INST] <s>\n",
      "\n",
      "I would be happy to help! Here is a 2-sentence, 5-character poem about the theme of 风雨，旅人 (Wind and Rain, Traveler):\n",
      "\n",
      "风雨过客客\n",
      "雨过客风客\n",
      "\n",
      "Translation:\n",
      "\"Wind and rain pass through the traveler,\n",
      "Rain passes through the wind, traveler.\"\n",
      "\n",
      "Please let me know if this meets your requirements!\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") \n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output does not make any sense. Not only the number of characters in each line is not suffcient to our requirement, but also the tune and words used is not like ancient poet at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing the training dataset\n",
    "\n",
    "Let's use sft to improve Llama3-8B-Instruct's ablity in this field now!\n",
    "\n",
    "You should prepare for the data we need to use for SFT in `02_poet data` .\n",
    "\n",
    "Please complete the procedures in that notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SFT with Llama-Factory\n",
    "\n",
    "For Processing SFT, we use llama factory, which is a highly modular, user-friendly platform with great ease of use, supporting distributed training and a variety of pre-trained models. Llama factory provide a WebUI to make it easy for using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-18 10:12:45,728] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-18 10:12:51 [__init__.py:239] Automatically detected platform cuda.\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "[2025-05-18 10:13:56,088] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-18 10:14:01 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|2025-05-18 10:14:04] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:48823\n",
      "W0518 10:14:06.654000 6766 torch/distributed/run.py:792] \n",
      "W0518 10:14:06.654000 6766 torch/distributed/run.py:792] *****************************************\n",
      "W0518 10:14:06.654000 6766 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0518 10:14:06.654000 6766 torch/distributed/run.py:792] *****************************************\n",
      "[2025-05-18 10:14:12,337] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:14:12,393] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:14:12,395] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:14:12,407] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-05-18 10:14:18,166] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:14:18,169] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:14:18,169] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-18 10:14:18,260] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:14:18,260] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[WARNING|2025-05-18 10:14:18] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[INFO|2025-05-18 10:14:18] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[INFO|2025-05-18 10:14:18] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:18,638 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:18,639 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:18,639 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:18,640 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:18,640 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:18,640 >> loading file chat_template.jinja\n",
      "[INFO|2025-05-18 10:14:18] llamafactory.hparams.parser:401 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-18 10:14:18] llamafactory.hparams.parser:401 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-18 10:14:18] llamafactory.hparams.parser:401 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-18 10:14:19,126 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:14:19,131 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:14:19,133 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:19,138 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:19,138 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:19,138 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:19,138 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:19,138 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:14:19,138 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-18 10:14:19,557 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-18 10:14:19] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-18 10:14:19] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-18 10:14:19] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-18 10:14:19] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|2025-05-18 10:14:19] llamafactory.data.loader:143 >> Loading dataset alpaca_sft_dataset_with_varied_instructions.json...\n",
      "[rank2]:[W518 10:14:19.113330759 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W518 10:14:19.126055731 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank3]:[W518 10:14:19.191839913 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W518 10:14:20.412247997 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 13347, 11, 499, 527, 264, 8620, 40360, 11, 649, 499, 3350, 264, 220, 20, 80325, 220, 19, 8614, 33894, 922, 279, 22100, 25, 80721, 237, 30867, 11, 122438, 9080, 11, 123335, 103125, 11, 10447, 3299, 51043, 242, 30, 128009, 128006, 78191, 128007, 271, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, you are a Chinese poet, can you write a 5-character 4-line poem about the themes: 屏开, 晴日, 春风, 绿苔?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "labels:\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:14:22,237 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:14:22,238 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-18 10:14:22] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2025-05-18 10:14:22] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|modeling_utils.py:1121] 2025-05-18 10:14:22,367 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-18 10:14:22,368 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-18 10:14:22,370 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards:   0%|                          | 0/4 [00:00<?, ?it/s]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.37s/it]\n",
      "\n",
      "[INFO|modeling_utils.py:4926] 2025-05-18 10:14:36,212 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-18 10:14:36,212 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-18 10:14:36,219 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-18 10:14:36,220 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.38s/it]\n",
      "[INFO|2025-05-18 10:14:36] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-18 10:14:36] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-18 10:14:36] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-18 10:14:36] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-05-18 10:14:36] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,v_proj,q_proj,k_proj,up_proj,down_proj,o_proj\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.43s/it]\n",
      "[INFO|2025-05-18 10:14:38] llamafactory.model.loader:143 >> trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
      "[INFO|trainer.py:748] 2025-05-18 10:14:38,220 >> Using auto half precision backend\n",
      "[2025-05-18 10:14:38,576] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-18 10:14:38,576] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-18 10:14:39,037] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-18 10:14:39,042] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-05-18 10:14:39,043] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-18 10:14:39,091] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-05-18 10:14:39,091] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'bitsandbytes.optim.adamw.AdamW'>\n",
      "[2025-05-18 10:14:39,091] [WARNING] [engine.py:1338:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2025-05-18 10:14:39,091] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-05-18 10:14:39,091] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000\n",
      "[2025-05-18 10:14:39,091] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000\n",
      "[2025-05-18 10:14:39,091] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: False\n",
      "[2025-05-18 10:14:39,091] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True\n",
      "[2025-05-18 10:14:40,655] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-18 10:14:40,656] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.86 GB         CA 5.93 GB         Max_CA 6 GB \n",
      "[2025-05-18 10:14:40,656] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.62 GB, percent = 3.5%\n",
      "[2025-05-18 10:14:40,873] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-18 10:14:40,874] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.94 GB         CA 6.08 GB         Max_CA 6 GB \n",
      "[2025-05-18 10:14:40,874] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.62 GB, percent = 3.5%\n",
      "[2025-05-18 10:14:40,874] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized\n",
      "[2025-05-18 10:14:41,089] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-18 10:14:41,090] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.78 GB         CA 6.08 GB         Max_CA 6 GB \n",
      "[2025-05-18 10:14:41,090] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.62 GB, percent = 3.5%\n",
      "[2025-05-18 10:14:41,094] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-05-18 10:14:41,094] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-18 10:14:41,094] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-18 10:14:41,094] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2025-05-18 10:14:41,101] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n",
      "[2025-05-18 10:14:41,101] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-18 10:14:41,101] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-18 10:14:41,101] [INFO] [config.py:1007:print]   amp_enabled .................. False\n",
      "[2025-05-18 10:14:41,101] [INFO] [config.py:1007:print]   amp_params ................... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0ba085cb80>\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   communication_data_type ...... None\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   disable_allgather ............ False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   dump_state ................... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None\n",
      "[2025-05-18 10:14:41,102] [INFO] [config.py:1007:print]   fp16_enabled ................. False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   global_rank .................. 0\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   gradient_clipping ............ 0.3\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   loss_scale ................... 1.0\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   optimizer_name ............... None\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   optimizer_params ............. None\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   pld_enabled .................. False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   pld_params ................... False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   scheduler_name ............... None\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   scheduler_params ............. None\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   sparse_attention ............. None\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   steps_per_print .............. inf\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   train_batch_size ............. 24\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  6\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n",
      "[2025-05-18 10:14:41,103] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-18 10:14:41,104] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n",
      "[2025-05-18 10:14:41,104] [INFO] [config.py:1007:print]   world_size ................... 4\n",
      "[2025-05-18 10:14:41,104] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True\n",
      "[2025-05-18 10:14:41,104] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-05-18 10:14:41,104] [INFO] [config.py:1007:print]   zero_enabled ................. True\n",
      "[2025-05-18 10:14:41,104] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-18 10:14:41,104] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2\n",
      "[2025-05-18 10:14:41,104] [INFO] [config.py:993:print_user_config]   json = {\n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 6, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 0.3, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 5.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"round_robin_gradients\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n",
      "[INFO|trainer.py:2414] 2025-05-18 10:14:41,105 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-05-18 10:14:41,105 >>   Num examples = 100,000\n",
      "[INFO|trainer.py:2416] 2025-05-18 10:14:41,105 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2417] 2025-05-18 10:14:41,105 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:2420] 2025-05-18 10:14:41,105 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2421] 2025-05-18 10:14:41,106 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2422] 2025-05-18 10:14:41,106 >>   Total optimization steps = 4,167\n",
      "[INFO|trainer.py:2423] 2025-05-18 10:14:41,110 >>   Number of trainable parameters = 167,772,160\n",
      "  1%|▎                                        | 36/4167 [00:23<43:08,  1.60it/s]Fatal Python error: Aborted\n",
      "\n",
      "Current thread 0x00007f6ecfce7740 (most recent call first):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 870 in _invoke_run\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 711 in run\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 137 in wrapper\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 260 in launch_agent\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138 in __call__\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909 in run\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918 in main\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355 in wrapper\n",
      "  File \"/usr/local/bin/torchrun\", line 8 in <module>\n",
      "\n",
      "Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special (total: 24)\n",
      "  1%|▎                                        | 37/4167 [00:23<43:33,  1.58it/s][INFO|trainer.py:2681] 2025-05-18 10:15:04,932 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 23.8225, 'train_samples_per_second': 4197.714, 'train_steps_per_second': 174.919, 'train_loss': 4.4176041886613175, 'epoch': 0.01, 'num_input_tokens_seen': 82944}\n",
      "  1%|▎                                        | 37/4167 [00:23<44:25,  1.55it/s]\n",
      "[INFO|trainer.py:3984] 2025-05-18 10:15:08,481 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-12-59\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:15:08,535 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:15:08,536 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:15:11,331 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-12-59/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:15:11,340 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-12-59/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.0089\n",
      "  num_input_tokens_seen    =      82944\n",
      "  total_flos               =  3556185GF\n",
      "  train_loss               =     4.4176\n",
      "  train_runtime            = 0:00:23.82\n",
      "  train_samples_per_second =   4197.714\n",
      "  train_steps_per_second   =    174.919\n",
      "[WARNING|2025-05-18 10:15:11] llamafactory.extras.ploting:148 >> No metric loss to plot.\n",
      "[WARNING|2025-05-18 10:15:11] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-05-18 10:15:11] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-05-18 10:15:11,896 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[2025-05-18 10:15:38,092] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-18 10:15:42 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|2025-05-18 10:15:46] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:34943\n",
      "W0518 10:15:48.299000 10029 torch/distributed/run.py:792] \n",
      "W0518 10:15:48.299000 10029 torch/distributed/run.py:792] *****************************************\n",
      "W0518 10:15:48.299000 10029 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0518 10:15:48.299000 10029 torch/distributed/run.py:792] *****************************************\n",
      "[2025-05-18 10:15:53,865] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:15:53,877] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:15:53,877] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:15:53,956] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-05-18 10:15:59,121] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:15:59,203] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:15:59,225] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:15:59,225] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-18 10:15:59,250] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[WARNING|2025-05-18 10:15:59] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[INFO|2025-05-18 10:15:59] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank1]:     launch()\n",
      "[rank1]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank1]:     run_exp()\n",
      "[rank1]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
      "[rank1]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank1]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 55, in _training_function\n",
      "[rank1]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "[rank1]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 373, in get_train_args\n",
      "[rank1]:     raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
      "[rank1]: ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank0]:     launch()\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank0]:     run_exp()\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
      "[rank0]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 55, in _training_function\n",
      "[rank0]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "[rank0]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 373, in get_train_args\n",
      "[rank0]:     raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
      "[rank0]: ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank2]:     launch()\n",
      "[rank2]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank2]:     run_exp()\n",
      "[rank2]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
      "[rank2]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank2]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 55, in _training_function\n",
      "[rank2]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "[rank2]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 373, in get_train_args\n",
      "[rank2]:     raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
      "[rank2]: ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank3]:     launch()\n",
      "[rank3]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank3]:     run_exp()\n",
      "[rank3]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
      "[rank3]:     _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "[rank3]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 55, in _training_function\n",
      "[rank3]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "[rank3]:   File \"/gfshome/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 373, in get_train_args\n",
      "[rank3]:     raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
      "[rank3]: ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
      "[rank0]:[W518 10:16:00.842802252 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0518 10:16:01.244000 10029 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 10217 closing signal SIGTERM\n",
      "W0518 10:16:01.244000 10029 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 10218 closing signal SIGTERM\n",
      "E0518 10:16:01.359000 10029 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 10215) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/gfshome/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-05-18_10:16:01\n",
      "  host      : s1318-pytorch-liukh-89b979f54-4z5x9\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 10216)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-05-18_10:16:01\n",
      "  host      : s1318-pytorch-liukh-89b979f54-4z5x9\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 10215)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/cli.py\", line 95, in main\n",
      "    process = subprocess.run(\n",
      "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '34943', '/gfshome/LLaMA-Factory/src/llamafactory/launcher.py', 'saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-12-59/training_args.yaml']' returned non-zero exit status 1.\n",
      "[2025-05-18 10:19:09,777] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-18 10:19:14 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|2025-05-18 10:19:18] llamafactory.cli:143 >> Initializing 4 distributed tasks at: 127.0.0.1:60887\n",
      "W0518 10:19:19.949000 18460 torch/distributed/run.py:792] \n",
      "W0518 10:19:19.949000 18460 torch/distributed/run.py:792] *****************************************\n",
      "W0518 10:19:19.949000 18460 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0518 10:19:19.949000 18460 torch/distributed/run.py:792] *****************************************\n",
      "[2025-05-18 10:19:25,496] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:19:25,557] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:19:25,594] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-18 10:19:25,601] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-05-18 10:19:31,127] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:19:31,128] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:19:31,128] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-18 10:19:31,238] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-18 10:19:31,238] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[WARNING|2025-05-18 10:19:31] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[INFO|2025-05-18 10:19:31] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[INFO|2025-05-18 10:19:31] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:31,523 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:31,524 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:31,524 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:31,524 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:31,524 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:31,525 >> loading file chat_template.jinja\n",
      "[INFO|2025-05-18 10:19:31] llamafactory.hparams.parser:401 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-18 10:19:31] llamafactory.hparams.parser:401 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|2025-05-18 10:19:31] llamafactory.hparams.parser:401 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-18 10:19:32,012 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:19:32,020 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:19:32,022 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:32,029 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:32,029 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:32,029 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:32,029 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:32,029 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 10:19:32,029 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-18 10:19:32,463 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-18 10:19:32] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-18 10:19:32] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-18 10:19:32] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-18 10:19:32] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[rank3]:[W518 10:19:32.995803309 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[INFO|2025-05-18 10:19:32] llamafactory.data.loader:143 >> Loading dataset alpaca_sft_dataset_with_varied_instructions.json...\n",
      "[rank2]:[W518 10:19:32.083431943 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W518 10:19:32.103113391 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W518 10:19:33.971890281 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 13347, 11, 499, 527, 264, 8620, 40360, 11, 649, 499, 3350, 264, 220, 20, 80325, 220, 19, 8614, 33894, 922, 279, 22100, 25, 80721, 237, 30867, 11, 122438, 9080, 11, 123335, 103125, 11, 10447, 3299, 51043, 242, 30, 128009, 128006, 78191, 128007, 271, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, you are a Chinese poet, can you write a 5-character 4-line poem about the themes: 屏开, 晴日, 春风, 绿苔?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "labels:\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:19:34,864 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:19:34,865 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-18 10:19:34] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2025-05-18 10:19:34] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|modeling_utils.py:1121] 2025-05-18 10:19:35,000 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-18 10:19:35,001 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-18 10:19:35,003 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "Loading checkpoint shards:   0%|                          | 0/4 [00:00<?, ?it/s]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.40s/it]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-18 10:19:48,734 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-18 10:19:48,735 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-18 10:19:48,743 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-18 10:19:48,743 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.36s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.36s/it]\n",
      "[INFO|2025-05-18 10:19:48] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-18 10:19:48] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-18 10:19:48] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-18 10:19:48] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-05-18 10:19:48] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,k_proj,up_proj,gate_proj,v_proj,down_proj,o_proj\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:13<00:00,  3.40s/it]\n",
      "[INFO|2025-05-18 10:19:50] llamafactory.model.loader:143 >> trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
      "[INFO|trainer.py:748] 2025-05-18 10:19:50,727 >> Using auto half precision backend\n",
      "[2025-05-18 10:19:51,086] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-18 10:19:51,087] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-18 10:19:51,411] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-18 10:19:51,416] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-05-18 10:19:51,416] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-18 10:19:51,457] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-05-18 10:19:51,457] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'bitsandbytes.optim.adamw.AdamW'>\n",
      "[2025-05-18 10:19:51,457] [WARNING] [engine.py:1338:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2025-05-18 10:19:51,457] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-05-18 10:19:51,457] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000\n",
      "[2025-05-18 10:19:51,457] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000\n",
      "[2025-05-18 10:19:51,457] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: False\n",
      "[2025-05-18 10:19:51,457] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True\n",
      "[2025-05-18 10:19:53,094] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-18 10:19:53,095] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.86 GB         CA 5.93 GB         Max_CA 6 GB \n",
      "[2025-05-18 10:19:53,095] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.71 GB, percent = 3.5%\n",
      "[2025-05-18 10:19:53,312] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-18 10:19:53,313] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.94 GB         CA 6.08 GB         Max_CA 6 GB \n",
      "[2025-05-18 10:19:53,313] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.71 GB, percent = 3.5%\n",
      "[2025-05-18 10:19:53,313] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized\n",
      "[2025-05-18 10:19:53,529] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-18 10:19:53,529] [INFO] [utils.py:782:see_memory_usage] MA 5.78 GB         Max_MA 5.78 GB         CA 6.08 GB         Max_CA 6 GB \n",
      "[2025-05-18 10:19:53,529] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 35.71 GB, percent = 3.5%\n",
      "[2025-05-18 10:19:53,533] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-05-18 10:19:53,533] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-18 10:19:53,533] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-18 10:19:53,533] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2025-05-18 10:19:53,540] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   amp_enabled .................. False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   amp_params ................... False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa7e4e10dc0>\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   communication_data_type ...... None\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   disable_allgather ............ False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   dump_state ................... False\n",
      "[2025-05-18 10:19:53,541] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   fp16_enabled ................. False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   global_rank .................. 0\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   gradient_clipping ............ 0.3\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   loss_scale ................... 1.0\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   optimizer_name ............... None\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   optimizer_params ............. None\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   pld_enabled .................. False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   pld_params ................... False\n",
      "[2025-05-18 10:19:53,542] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   scheduler_name ............... None\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   scheduler_params ............. None\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   sparse_attention ............. None\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   steps_per_print .............. inf\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   train_batch_size ............. 32\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  8\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   world_size ................... 4\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   zero_enabled ................. True\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2\n",
      "[2025-05-18 10:19:53,543] [INFO] [config.py:993:print_user_config]   json = {\n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 0.3, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 5.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"round_robin_gradients\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n",
      "[INFO|trainer.py:2414] 2025-05-18 10:19:53,545 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-05-18 10:19:53,545 >>   Num examples = 100,000\n",
      "[INFO|trainer.py:2416] 2025-05-18 10:19:53,545 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2417] 2025-05-18 10:19:53,545 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2420] 2025-05-18 10:19:53,545 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2421] 2025-05-18 10:19:53,545 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2422] 2025-05-18 10:19:53,545 >>   Total optimization steps = 3,125\n",
      "[INFO|trainer.py:2423] 2025-05-18 10:19:53,550 >>   Number of trainable parameters = 167,772,160\n",
      "  3%|█▎                                      | 100/3125 [01:02<31:08,  1.62it/s][INFO|2025-05-18 10:20:56] llamafactory.train.callbacks:143 >> {'loss': 3.8079, 'learning_rate': 9.9000e-05, 'epoch': 0.03, 'throughput': 4805.08}\n",
      "{'loss': 3.8079, 'grad_norm': 0.6693938374519348, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.03, 'num_input_tokens_seen': 301440}\n",
      "  6%|██▌                                     | 200/3125 [02:04<30:25,  1.60it/s][INFO|2025-05-18 10:21:58] llamafactory.train.callbacks:143 >> {'loss': 3.2825, 'learning_rate': 1.9900e-04, 'epoch': 0.06, 'throughput': 4828.33}\n",
      "{'loss': 3.2825, 'grad_norm': 0.5920405387878418, 'learning_rate': 0.000199, 'epoch': 0.06, 'num_input_tokens_seen': 603136}\n",
      "  6%|██▌                                     | 200/3125 [02:04<30:25,  1.60it/s][INFO|trainer.py:3984] 2025-05-18 10:22:02,014 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:22:02,087 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:22:02,089 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:22:05,040 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:22:05,048 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200/special_tokens_map.json\n",
      "[2025-05-18 10:22:05,664] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!\n",
      "[2025-05-18 10:22:05,737] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200/global_step200/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:22:05,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200/global_step200/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:22:10,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200/global_step200/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:22:10,177] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:22:16,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:22:16,758] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:22:16,758] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!\n",
      " 10%|███▊                                    | 300/3125 [03:26<29:04,  1.62it/s][INFO|2025-05-18 10:23:19] llamafactory.train.callbacks:143 >> {'loss': 3.1934, 'learning_rate': 1.9944e-04, 'epoch': 0.10, 'throughput': 4387.31}\n",
      "{'loss': 3.1934, 'grad_norm': 0.4924700856208801, 'learning_rate': 0.00019943522017712358, 'epoch': 0.1, 'num_input_tokens_seen': 904704}\n",
      " 13%|█████                                   | 400/3125 [04:28<28:13,  1.61it/s][INFO|2025-05-18 10:24:22] llamafactory.train.callbacks:143 >> {'loss': 3.1140, 'learning_rate': 1.9772e-04, 'epoch': 0.13, 'throughput': 4494.13}\n",
      "{'loss': 3.114, 'grad_norm': 0.539124608039856, 'learning_rate': 0.00019772453473338749, 'epoch': 0.13, 'num_input_tokens_seen': 1207104}\n",
      " 13%|█████                                   | 400/3125 [04:28<28:13,  1.61it/s][INFO|trainer.py:3984] 2025-05-18 10:24:25,363 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:24:25,435 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:24:25,436 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:24:28,279 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:24:28,288 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400/special_tokens_map.json\n",
      "[2025-05-18 10:24:28,760] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!\n",
      "[2025-05-18 10:24:28,860] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400/global_step400/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:24:28,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400/global_step400/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:24:33,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400/global_step400/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:24:33,147] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:24:39,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:24:40,000] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:24:40,000] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!\n",
      " 16%|██████▍                                 | 500/3125 [05:48<27:12,  1.61it/s][INFO|2025-05-18 10:25:42] llamafactory.train.callbacks:143 >> {'loss': 3.0481, 'learning_rate': 1.9489e-04, 'epoch': 0.16, 'throughput': 4324.12}\n",
      "{'loss': 3.0481, 'grad_norm': 0.4962666928768158, 'learning_rate': 0.00019488760116444966, 'epoch': 0.16, 'num_input_tokens_seen': 1508672}\n",
      " 19%|███████▋                                | 600/3125 [06:51<26:12,  1.61it/s][INFO|2025-05-18 10:26:44] llamafactory.train.callbacks:143 >> {'loss': 3.0075, 'learning_rate': 1.9096e-04, 'epoch': 0.19, 'throughput': 4404.15}\n",
      "{'loss': 3.0075, 'grad_norm': 0.5160715579986572, 'learning_rate': 0.0001909571143418903, 'epoch': 0.19, 'num_input_tokens_seen': 1811200}\n",
      " 19%|███████▋                                | 600/3125 [06:51<26:12,  1.61it/s][INFO|trainer.py:3984] 2025-05-18 10:26:46,835 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:26:46,899 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:26:46,900 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:26:49,897 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:26:49,906 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600/special_tokens_map.json\n",
      "[2025-05-18 10:26:50,366] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!\n",
      "[2025-05-18 10:26:50,442] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600/global_step600/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:26:50,442] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600/global_step600/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:26:54,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600/global_step600/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:26:54,718] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:27:01,390] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:27:01,448] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:27:01,448] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!\n",
      " 22%|████████▉                               | 700/3125 [08:10<25:04,  1.61it/s][INFO|2025-05-18 10:28:03] llamafactory.train.callbacks:143 >> {'loss': 2.9600, 'learning_rate': 1.8598e-04, 'epoch': 0.22, 'throughput': 4307.78}\n",
      "{'loss': 2.96, 'grad_norm': 0.4764446020126343, 'learning_rate': 0.0001859783720348634, 'epoch': 0.22, 'num_input_tokens_seen': 2112512}\n",
      " 26%|██████████▏                             | 800/3125 [09:12<23:58,  1.62it/s][INFO|2025-05-18 10:29:06] llamafactory.train.callbacks:143 >> {'loss': 2.9282, 'learning_rate': 1.8001e-04, 'epoch': 0.26, 'throughput': 4367.07}\n",
      "{'loss': 2.9282, 'grad_norm': 0.5229578018188477, 'learning_rate': 0.00018000875286588633, 'epoch': 0.26, 'num_input_tokens_seen': 2414144}\n",
      " 26%|██████████▏                             | 800/3125 [09:12<23:58,  1.62it/s][INFO|trainer.py:3984] 2025-05-18 10:29:08,439 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:29:08,507 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:29:08,508 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:29:11,510 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:29:11,519 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800/special_tokens_map.json\n",
      "[2025-05-18 10:29:11,948] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!\n",
      "[2025-05-18 10:29:12,020] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800/global_step800/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:29:12,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800/global_step800/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:29:16,101] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800/global_step800/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:29:16,118] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:29:22,840] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:29:22,869] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:29:22,869] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!\n",
      " 29%|███████████▌                            | 900/3125 [10:31<23:19,  1.59it/s][INFO|2025-05-18 10:30:25] llamafactory.train.callbacks:143 >> {'loss': 2.8942, 'learning_rate': 1.7312e-04, 'epoch': 0.29, 'throughput': 4297.67}\n",
      "{'loss': 2.8942, 'grad_norm': 0.5055446028709412, 'learning_rate': 0.000173117055038149, 'epoch': 0.29, 'num_input_tokens_seen': 2715648}\n",
      " 32%|████████████▍                          | 1000/3125 [11:34<22:01,  1.61it/s][INFO|2025-05-18 10:31:28] llamafactory.train.callbacks:143 >> {'loss': 2.8775, 'learning_rate': 1.6538e-04, 'epoch': 0.32, 'throughput': 4346.26}\n",
      "{'loss': 2.8775, 'grad_norm': 0.5224336981773376, 'learning_rate': 0.00016538270345532708, 'epoch': 0.32, 'num_input_tokens_seen': 3018304}\n",
      " 32%|████████████▍                          | 1000/3125 [11:34<22:01,  1.61it/s][INFO|trainer.py:3984] 2025-05-18 10:31:30,140 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:31:30,212 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:31:30,213 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:31:33,205 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:31:33,214 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000/special_tokens_map.json\n",
      "[2025-05-18 10:31:33,698] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!\n",
      "[2025-05-18 10:31:33,813] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:31:33,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:31:38,157] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:31:38,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:31:44,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:31:44,754] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:31:44,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!\n",
      " 35%|█████████████▋                         | 1100/3125 [12:54<21:20,  1.58it/s][INFO|2025-05-18 10:32:48] llamafactory.train.callbacks:143 >> {'loss': 2.8459, 'learning_rate': 1.5689e-04, 'epoch': 0.35, 'throughput': 4284.62}\n",
      "{'loss': 2.8459, 'grad_norm': 0.5278623700141907, 'learning_rate': 0.00015689483437162293, 'epoch': 0.35, 'num_input_tokens_seen': 3319680}\n",
      " 38%|██████████████▉                        | 1200/3125 [13:57<20:10,  1.59it/s][INFO|2025-05-18 10:33:51] llamafactory.train.callbacks:143 >> {'loss': 2.8527, 'learning_rate': 1.4775e-04, 'epoch': 0.38, 'throughput': 4322.19}\n",
      "{'loss': 2.8527, 'grad_norm': 0.5367806553840637, 'learning_rate': 0.00014775126812118864, 'epoch': 0.38, 'num_input_tokens_seen': 3621632}\n",
      " 38%|██████████████▉                        | 1200/3125 [13:57<20:10,  1.59it/s][INFO|trainer.py:3984] 2025-05-18 10:33:54,150 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:33:54,222 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:33:54,223 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:33:57,428 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:33:57,436 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200/special_tokens_map.json\n",
      "[2025-05-18 10:33:57,921] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!\n",
      "[2025-05-18 10:33:57,998] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:33:57,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:34:02,362] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200/global_step1200/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:34:02,379] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:34:09,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:34:09,174] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1200/global_step1200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:34:09,174] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!\n",
      " 42%|████████████████▏                      | 1300/3125 [15:18<18:49,  1.62it/s][INFO|2025-05-18 10:35:11] llamafactory.train.callbacks:143 >> {'loss': 2.8231, 'learning_rate': 1.3806e-04, 'epoch': 0.42, 'throughput': 4272.21}\n",
      "{'loss': 2.8231, 'grad_norm': 0.5272215604782104, 'learning_rate': 0.0001380573817659392, 'epoch': 0.42, 'num_input_tokens_seen': 3923392}\n",
      " 45%|█████████████████▍                     | 1400/3125 [16:20<17:46,  1.62it/s][INFO|2025-05-18 10:36:13] llamafactory.train.callbacks:143 >> {'loss': 2.8099, 'learning_rate': 1.2792e-04, 'epoch': 0.45, 'throughput': 4311.24}\n",
      "{'loss': 2.8099, 'grad_norm': 0.5291479229927063, 'learning_rate': 0.00012792489465417644, 'epoch': 0.45, 'num_input_tokens_seen': 4226688}\n",
      " 45%|█████████████████▍                     | 1400/3125 [16:20<17:46,  1.62it/s][INFO|trainer.py:3984] 2025-05-18 10:36:16,030 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:36:16,099 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:36:16,100 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:36:18,945 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:36:18,954 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400/special_tokens_map.json\n",
      "[2025-05-18 10:36:19,429] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1400 is about to be saved!\n",
      "[2025-05-18 10:36:19,513] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:36:19,513] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:36:23,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400/global_step1400/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:36:23,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:36:30,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:36:30,799] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1400/global_step1400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:36:30,799] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!\n",
      " 48%|██████████████████▋                    | 1500/3125 [17:40<16:48,  1.61it/s][INFO|2025-05-18 10:37:33] llamafactory.train.callbacks:143 >> {'loss': 2.7882, 'learning_rate': 1.1747e-04, 'epoch': 0.48, 'throughput': 4270.50}\n",
      "{'loss': 2.7882, 'grad_norm': 0.5308405160903931, 'learning_rate': 0.00011747058088612388, 'epoch': 0.48, 'num_input_tokens_seen': 4528448}\n",
      " 51%|███████████████████▉                   | 1600/3125 [18:42<15:47,  1.61it/s][INFO|2025-05-18 10:38:35] llamafactory.train.callbacks:143 >> {'loss': 2.7800, 'learning_rate': 1.0681e-04, 'epoch': 0.51, 'throughput': 4304.57}\n",
      "{'loss': 2.78, 'grad_norm': 0.5283554792404175, 'learning_rate': 0.00010681492352484919, 'epoch': 0.51, 'num_input_tokens_seen': 4830656}\n",
      " 51%|███████████████████▉                   | 1600/3125 [18:42<15:47,  1.61it/s][INFO|trainer.py:3984] 2025-05-18 10:38:37,872 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:38:37,950 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:38:37,951 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:38:40,876 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:38:40,885 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600/special_tokens_map.json\n",
      "[2025-05-18 10:38:41,338] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1600 is about to be saved!\n",
      "[2025-05-18 10:38:41,429] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:38:41,429] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:38:45,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600/global_step1600/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:38:45,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:38:52,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:38:52,211] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1600/global_step1600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:38:52,211] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!\n",
      " 54%|█████████████████████▏                 | 1700/3125 [20:00<14:39,  1.62it/s][INFO|2025-05-18 10:39:54] llamafactory.train.callbacks:143 >> {'loss': 2.7624, 'learning_rate': 9.6081e-05, 'epoch': 0.54, 'throughput': 4274.65}\n",
      "{'loss': 2.7624, 'grad_norm': 0.5162181854248047, 'learning_rate': 9.608072606241982e-05, 'epoch': 0.54, 'num_input_tokens_seen': 5133824}\n",
      " 58%|██████████████████████▍                | 1800/3125 [21:03<13:45,  1.60it/s][INFO|2025-05-18 10:40:56] llamafactory.train.callbacks:143 >> {'loss': 2.7449, 'learning_rate': 8.5392e-05, 'epoch': 0.58, 'throughput': 4302.67}\n",
      "{'loss': 2.7449, 'grad_norm': 0.5340375900268555, 'learning_rate': 8.539169714375885e-05, 'epoch': 0.58, 'num_input_tokens_seen': 5435520}\n",
      " 58%|██████████████████████▍                | 1800/3125 [21:03<13:45,  1.60it/s][INFO|trainer.py:3984] 2025-05-18 10:40:58,951 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:40:59,021 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:40:59,023 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:41:01,994 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:41:02,002 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800/special_tokens_map.json\n",
      "[2025-05-18 10:41:02,536] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1800 is about to be saved!\n",
      "[2025-05-18 10:41:02,612] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:41:02,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:41:06,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800/global_step1800/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:41:06,918] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:41:13,575] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:41:13,650] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-1800/global_step1800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:41:13,650] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!\n",
      " 61%|███████████████████████▋               | 1900/3125 [22:22<12:32,  1.63it/s][INFO|2025-05-18 10:42:16] llamafactory.train.callbacks:143 >> {'loss': 2.7437, 'learning_rate': 7.4871e-05, 'epoch': 0.61, 'throughput': 4272.93}\n",
      "{'loss': 2.7437, 'grad_norm': 0.5401281714439392, 'learning_rate': 7.487102485886552e-05, 'epoch': 0.61, 'num_input_tokens_seen': 5736832}\n",
      " 64%|████████████████████████▉              | 2000/3125 [23:24<11:38,  1.61it/s][INFO|2025-05-18 10:43:18] llamafactory.train.callbacks:143 >> {'loss': 2.7244, 'learning_rate': 6.4640e-05, 'epoch': 0.64, 'throughput': 4298.49}\n",
      "{'loss': 2.7244, 'grad_norm': 0.549220860004425, 'learning_rate': 6.463995703428662e-05, 'epoch': 0.64, 'num_input_tokens_seen': 6037504}\n",
      " 64%|████████████████████████▉              | 2000/3125 [23:24<11:38,  1.61it/s][INFO|trainer.py:3984] 2025-05-18 10:43:20,187 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:43:20,251 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:43:20,252 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:43:23,160 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:43:23,171 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000/special_tokens_map.json\n",
      "[2025-05-18 10:43:23,650] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!\n",
      "[2025-05-18 10:43:23,744] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:43:23,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:43:28,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:43:28,103] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:43:34,704] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:43:34,766] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:43:34,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!\n",
      " 67%|██████████████████████████▏            | 2100/3125 [24:44<10:50,  1.58it/s][INFO|2025-05-18 10:44:38] llamafactory.train.callbacks:143 >> {'loss': 2.7169, 'learning_rate': 5.4816e-05, 'epoch': 0.67, 'throughput': 4269.42}\n",
      "{'loss': 2.7169, 'grad_norm': 0.5662442445755005, 'learning_rate': 5.481640388558551e-05, 'epoch': 0.67, 'num_input_tokens_seen': 6339328}\n",
      " 70%|███████████████████████████▍           | 2200/3125 [25:48<09:43,  1.58it/s][INFO|2025-05-18 10:45:41] llamafactory.train.callbacks:143 >> {'loss': 2.7181, 'learning_rate': 4.5514e-05, 'epoch': 0.70, 'throughput': 4290.21}\n",
      "{'loss': 2.7181, 'grad_norm': 0.5525920391082764, 'learning_rate': 4.5513579134850816e-05, 'epoch': 0.7, 'num_input_tokens_seen': 6641408}\n",
      " 70%|███████████████████████████▍           | 2200/3125 [25:48<09:43,  1.58it/s][INFO|trainer.py:3984] 2025-05-18 10:45:44,292 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:45:44,341 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:45:44,342 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:45:47,560 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:45:47,569 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200/special_tokens_map.json\n",
      "[2025-05-18 10:45:48,047] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2200 is about to be saved!\n",
      "[2025-05-18 10:45:48,126] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:45:48,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:45:52,516] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200/global_step2200/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:45:52,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:45:58,955] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:45:58,987] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2200/global_step2200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:45:58,987] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2200 is ready now!\n",
      " 74%|████████████████████████████▋          | 2300/3125 [27:08<08:41,  1.58it/s][INFO|2025-05-18 10:47:02] llamafactory.train.callbacks:143 >> {'loss': 2.6908, 'learning_rate': 3.6839e-05, 'epoch': 0.74, 'throughput': 4262.64}\n",
      "{'loss': 2.6908, 'grad_norm': 0.5862298607826233, 'learning_rate': 3.6838695253988206e-05, 'epoch': 0.74, 'num_input_tokens_seen': 6942784}\n",
      " 77%|█████████████████████████████▉         | 2400/3125 [28:11<07:31,  1.61it/s][INFO|2025-05-18 10:48:04] llamafactory.train.callbacks:143 >> {'loss': 2.6808, 'learning_rate': 2.8892e-05, 'epoch': 0.77, 'throughput': 4284.50}\n",
      "{'loss': 2.6808, 'grad_norm': 0.549959659576416, 'learning_rate': 2.8891727870753983e-05, 'epoch': 0.77, 'num_input_tokens_seen': 7245376}\n",
      " 77%|█████████████████████████████▉         | 2400/3125 [28:11<07:31,  1.61it/s][INFO|trainer.py:3984] 2025-05-18 10:48:06,819 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:48:06,884 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:48:06,885 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:48:09,966 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:48:09,975 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400/special_tokens_map.json\n",
      "[2025-05-18 10:48:10,457] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2400 is about to be saved!\n",
      "[2025-05-18 10:48:10,545] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:48:10,545] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:48:14,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400/global_step2400/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:48:14,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:48:21,580] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:48:21,624] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2400/global_step2400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:48:21,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!\n",
      " 80%|███████████████████████████████▏       | 2500/3125 [29:32<06:33,  1.59it/s][INFO|2025-05-18 10:49:25] llamafactory.train.callbacks:143 >> {'loss': 2.6589, 'learning_rate': 2.1764e-05, 'epoch': 0.80, 'throughput': 4258.99}\n",
      "{'loss': 2.6589, 'grad_norm': 0.552990198135376, 'learning_rate': 2.1764263577407896e-05, 'epoch': 0.8, 'num_input_tokens_seen': 7547008}\n",
      " 83%|████████████████████████████████▍      | 2600/3125 [30:34<05:29,  1.59it/s][INFO|2025-05-18 10:50:28] llamafactory.train.callbacks:143 >> {'loss': 2.6679, 'learning_rate': 1.5538e-05, 'epoch': 0.83, 'throughput': 4277.47}\n",
      "{'loss': 2.6679, 'grad_norm': 0.5698215365409851, 'learning_rate': 1.553844442067066e-05, 'epoch': 0.83, 'num_input_tokens_seen': 7849152}\n",
      " 83%|████████████████████████████████▍      | 2600/3125 [30:34<05:29,  1.59it/s][INFO|trainer.py:3984] 2025-05-18 10:50:31,223 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:50:31,298 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:50:31,300 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:50:34,518 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:50:34,527 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600/special_tokens_map.json\n",
      "[2025-05-18 10:50:35,018] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2600 is about to be saved!\n",
      "[2025-05-18 10:50:35,101] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600/global_step2600/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:50:35,102] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600/global_step2600/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:50:39,486] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600/global_step2600/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:50:39,543] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:50:46,031] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:50:46,103] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2600/global_step2600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:50:46,103] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2600 is ready now!\n",
      " 86%|█████████████████████████████████▋     | 2700/3125 [31:55<04:24,  1.61it/s][INFO|2025-05-18 10:51:48] llamafactory.train.callbacks:143 >> {'loss': 2.6653, 'learning_rate': 1.0286e-05, 'epoch': 0.86, 'throughput': 4256.42}\n",
      "{'loss': 2.6653, 'grad_norm': 0.555202841758728, 'learning_rate': 1.0286021237445697e-05, 'epoch': 0.86, 'num_input_tokens_seen': 8151168}\n",
      " 90%|██████████████████████████████████▉    | 2800/3125 [32:57<03:22,  1.60it/s][INFO|2025-05-18 10:52:50] llamafactory.train.callbacks:143 >> {'loss': 2.6745, 'learning_rate': 6.0675e-06, 'epoch': 0.90, 'throughput': 4275.35}\n",
      "{'loss': 2.6745, 'grad_norm': 0.5559124946594238, 'learning_rate': 6.067526746348751e-06, 'epoch': 0.9, 'num_input_tokens_seen': 8453568}\n",
      " 90%|██████████████████████████████████▉    | 2800/3125 [32:57<03:22,  1.60it/s][INFO|trainer.py:3984] 2025-05-18 10:52:52,902 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:52:52,967 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:52:52,968 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:52:56,127 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:52:56,137 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800/special_tokens_map.json\n",
      "[2025-05-18 10:52:56,629] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2800 is about to be saved!\n",
      "[2025-05-18 10:52:56,712] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800/global_step2800/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:52:56,713] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800/global_step2800/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:53:00,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800/global_step2800/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:53:01,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800/global_step2800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:53:07,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800/global_step2800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:53:07,573] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-2800/global_step2800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:53:07,573] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2800 is ready now!\n",
      " 93%|████████████████████████████████████▏  | 2900/3125 [34:16<02:19,  1.61it/s][INFO|2025-05-18 10:54:10] llamafactory.train.callbacks:143 >> {'loss': 2.6719, 'learning_rate': 2.9316e-06, 'epoch': 0.93, 'throughput': 4257.80}\n",
      "{'loss': 2.6719, 'grad_norm': 0.559188187122345, 'learning_rate': 2.9315779249355356e-06, 'epoch': 0.93, 'num_input_tokens_seen': 8756096}\n",
      " 96%|█████████████████████████████████████▍ | 3000/3125 [35:18<01:17,  1.61it/s][INFO|2025-05-18 10:55:12] llamafactory.train.callbacks:143 >> {'loss': 2.6551, 'learning_rate': 9.1432e-07, 'epoch': 0.96, 'throughput': 4275.78}\n",
      "{'loss': 2.6551, 'grad_norm': 0.5367501378059387, 'learning_rate': 9.143157125359514e-07, 'epoch': 0.96, 'num_input_tokens_seen': 9058432}\n",
      " 96%|█████████████████████████████████████▍ | 3000/3125 [35:18<01:17,  1.61it/s][INFO|trainer.py:3984] 2025-05-18 10:55:14,148 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:55:14,216 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:55:14,217 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:55:17,318 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:55:17,327 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000/special_tokens_map.json\n",
      "[2025-05-18 10:55:17,795] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!\n",
      "[2025-05-18 10:55:17,895] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:55:17,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:55:22,113] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:55:22,161] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000/global_step3000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:55:28,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000/global_step3000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:55:28,835] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3000/global_step3000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:55:28,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!\n",
      " 99%|██████████████████████████████████████▋| 3100/3125 [36:39<00:15,  1.58it/s][INFO|2025-05-18 10:56:32] llamafactory.train.callbacks:143 >> {'loss': 2.6691, 'learning_rate': 3.8988e-08, 'epoch': 0.99, 'throughput': 4256.01}\n",
      "{'loss': 2.6691, 'grad_norm': 0.5466742515563965, 'learning_rate': 3.898849596456478e-08, 'epoch': 0.99, 'num_input_tokens_seen': 9359488}\n",
      "100%|███████████████████████████████████████| 3125/3125 [36:54<00:00,  1.58it/s][INFO|trainer.py:3984] 2025-05-18 10:56:51,118 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:56:51,187 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:56:51,188 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:56:54,326 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:56:54,335 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125/special_tokens_map.json\n",
      "[2025-05-18 10:56:54,786] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step3125 is about to be saved!\n",
      "[2025-05-18 10:56:54,913] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125/global_step3125/mp_rank_00_model_states.pt\n",
      "[2025-05-18 10:56:54,913] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125/global_step3125/mp_rank_00_model_states.pt...\n",
      "[2025-05-18 10:56:59,128] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125/global_step3125/mp_rank_00_model_states.pt.\n",
      "[2025-05-18 10:56:59,181] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125/global_step3125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-18 10:57:05,888] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125/global_step3125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-18 10:57:05,941] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/checkpoint-3125/global_step3125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-18 10:57:05,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3125 is ready now!\n",
      "[INFO|trainer.py:2681] 2025-05-18 10:57:05,997 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2232.4478, 'train_samples_per_second': 44.794, 'train_steps_per_second': 1.4, 'train_loss': 2.8519361328125, 'epoch': 1.0, 'num_input_tokens_seen': 9434944}\n",
      "100%|███████████████████████████████████████| 3125/3125 [37:12<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:3984] 2025-05-18 10:57:08,396 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 10:57:08,446 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 10:57:08,447 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 10:57:11,542 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 10:57:11,551 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =         1.0\n",
      "  num_input_tokens_seen    =     9434944\n",
      "  total_flos               = 404518845GF\n",
      "  train_loss               =      2.8519\n",
      "  train_runtime            =  0:37:12.44\n",
      "  train_samples_per_second =      44.794\n",
      "  train_steps_per_second   =         1.4\n",
      "Figure saved at: saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18/training_loss.png\n",
      "[WARNING|2025-05-18 10:57:12] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-05-18 10:57:12] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-05-18 10:57:12,206 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2997, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/cli.py\", line 115, in main\n",
      "    COMMAND_MAP[command]()\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 97, in run_web_ui\n",
      "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2903, in launch\n",
      "    self.block_thread()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 3001, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 69, in close\n",
      "    self.thread.join(timeout=5)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1100, in join\n",
      "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "!cd LLaMA-Factory && llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the training parameters we selected in the file `Llama3-8B-Instruct-sft.yaml`, or refer to the screenshot in the slides. After you fullfill the parameters, click `Start` and wait for the SFT process to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training runs to complete, please paste your loss change chat below. \n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss Curve](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2025-05-17-23-53-54  train_2025-05-18-10-12-59\n",
      "train_2025-05-18-00-35-16  train_2025-05-18-10-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# You can now terminate the training process by stopping the previous cell.\n",
    "# The resulting LoRA is saved in LLaMA-Factory/saves/Llama-3-8B-Instruct/lora \n",
    "# (who is automatically named with a date as suffix)\n",
    "!ls LLaMA-Factory/saves/Llama-3-8B-Instruct/lora "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the LoRA into the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-18 11:19:41,175] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-18 11:19:46 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:49,950 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:49,950 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:49,950 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:49,950 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:49,950 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:49,950 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-18 11:19:50,344 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 11:19:50,352 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 11:19:50,353 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:50,361 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:50,361 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:50,361 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:50,361 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:50,361 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-18 11:19:50,361 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-18 11:19:50,753 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-18 11:19:50] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-18 11:19:50] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-18 11:19:50] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-18 11:19:50] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|configuration_utils.py:691] 2025-05-18 11:19:50,775 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-18 11:19:50,776 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-18 11:19:50] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:1121] 2025-05-18 11:19:50,847 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-18 11:19:50,847 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-18 11:19:50,870 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00, 46.70it/s]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-18 11:19:50,994 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-18 11:19:50,994 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-18 11:19:50,998 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-18 11:19:50,998 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-18 11:19:51] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|2025-05-18 11:21:25] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
      "[INFO|2025-05-18 11:21:25] llamafactory.model.adapter:143 >> Loaded adapter(s): /gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-18-10-18\n",
      "[INFO|2025-05-18 11:21:25] llamafactory.model.loader:143 >> all params: 8,030,261,248\n",
      "[INFO|2025-05-18 11:21:25] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
      "[INFO|configuration_utils.py:419] 2025-05-18 11:21:25,776 >> Configuration saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-05-18 11:21:25,785 >> Configuration saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/generation_config.json\n",
      "[INFO|modeling_utils.py:3580] 2025-05-18 11:23:39,574 >> The model is bigger than the maximum size per checkpoint (4GB) and is going to be split in 5 checkpoint shards. You can find where each parameters has been saved in the index located at /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-18 11:23:39,592 >> tokenizer config file saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-18 11:23:39,600 >> Special tokens file saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/special_tokens_map.json\n",
      "[INFO|2025-05-18 11:23:39] llamafactory.train.tuner:143 >> Ollama modelfile saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/Modelfile\n"
     ]
    }
   ],
   "source": [
    "# Merge Lora_model with Base model and save the merged model\n",
    "# ***Update the Lora-Merge.yaml configuration file and fullfill the Lora Path***\n",
    "# For more options in export, please refer to the [Llama-Factory Documentation](https://github.com/hiyouga/LLaMA-Factory/blob/main/docs/export.md)\n",
    "\n",
    "!llamafactory-cli export Lora_Merge.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testing the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose your Finetuneed model for test\n",
    "#Dont't forget to change the model name to your export_dir\n",
    "model_name = \"/gfshome/merged_model/Llama-3-8B-Instruct-sft-poet\"  # your new model \n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e9df4e3b8a48f9851748358d5cfa04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you don't want to merge your lora to get a new model, you can just using the lora when inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import transformers\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n",
    "# device_map = \"auto\"\n",
    "# adapter_name_or_path = \"/gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-15-09-25-23\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "#     bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "#     bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "#     bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    "# )\n",
    "\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     TrainingArguments,\n",
    "#     pipeline,\n",
    "# )\n",
    "# from peft import PeftModel\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=device_map\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     adapter_name_or_path, \n",
    "#     device_map=device_map\n",
    "# )\n",
    "\n",
    "# os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人? [/INST]>\n",
      "\n",
      "旅人心自苦，\n",
      "风雨夜漫漫。\n",
      "何时为此去，\n",
      "此去有无间。[/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST]\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: 花开, 桃源. [/INST]>\n",
      "\n",
      "花开无数桃源路，\n",
      "花落无情杨柳风。\n",
      "为问春来几番雨，\n",
      "无情无意只花红。[/INST] <s>桃源路在何处?</s>花开花落只花红。[/INST] <s>花开\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: 花开, 桃源.\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of 美国，关税? [/INST]>\n",
      "\n",
      "美国关税尽宜公，\n",
      "莫嫌官吏太无穷。\n",
      "公来不用嫌官吏，\n",
      "官吏如今只管公。[/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of 美国，关税?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  # 如果 tokenizer 支持这个 token\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
